{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzR_6g838ojv",
        "outputId": "4c5458e8-ce12-4a95-f22f-b7f8d17c2f92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cc0a8585cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "metadata": {
        "id": "oc64_iB08yej"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import FastText\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec_output_file = '/content/drive/MyDrive/NLP/pre-trained_embeddings/fasttext_word2vec.txt'\n",
        "fasttext_embed = KeyedVectors.load_word2vec_format(word2vec_output_file)"
      ],
      "metadata": {
        "id": "U9SuqSrX848V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, glove_embedding):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.ix_to_tag = {ix: tag for tag, ix in tag_to_ix.items()}\n",
        "\n",
        "#         self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "#                             num_layers=1, bidirectional=True)\n",
        "#         self.word_embeds = Word2VecEmbedding(word_to_vec_map, len(word_to_vec_map.index_to_key), word_to_vec_map.vector_size)\n",
        "#         self.lstm = nn.LSTM(word_to_vec_map.vector_size, hidden_dim // 2,\n",
        "#                             num_layers=1, bidirectional=True)\n",
        "        #self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(word_to_vec_map.vectors))\n",
        "        #self.word_embeds.weight.requires_grad = False  # freeze the embedding layer\n",
        "\n",
        "        #self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(glove_embedding.vectors))\n",
        "        #self.word_embeds.weight.requires_grad = False  # freeze the embedding layer\n",
        "\n",
        "        self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(fasttext_embed.vectors))\n",
        "        self.word_embeds.weight.requires_grad = False  # freeze the embedding layer\n",
        "\n",
        "        self.lstm = nn.LSTM(fasttext_embed.vector_size, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "#     def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "#         # Get the emission scores from the BiLSTM\n",
        "#         lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "#         # Find the best path, given the features.\n",
        "#         score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "#         return score, tag_seq\n",
        "    def forward(self, sentence):\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "        score, tag_indices = self._viterbi_decode(lstm_feats)\n",
        "\n",
        "        # Convert tag indices to actual tags\n",
        "        tags = [self.ix_to_tag[idx] for idx in tag_indices]\n",
        "        return score, tags"
      ],
      "metadata": {
        "id": "D0EsLawU8-6-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import json\n",
        "\n",
        "filename = '/content/drive/MyDrive/NLP/pre-trained_embeddings/NER_train.json'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    data_train = json.load(file)\n",
        "\n",
        "filename = '/content/drive/MyDrive/NLP/pre-trained_embeddings/NER_test.json'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    data_test = json.load(file)\n",
        "\n",
        "filename = '/content/drive/MyDrive/NLP/pre-trained_embeddings/NER_val.json'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    data_val = json.load(file)'''\n",
        "\n",
        "import json\n",
        "\n",
        "filename = '/content/drive/MyDrive/NLP/Assignment-2_laptop/ATE_train.json'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    data_train = json.load(file)\n",
        "\n",
        "filename = '/content/drive/MyDrive/NLP/Assignment-2_laptop/ATE_test.json'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    data_test = json.load(file)\n",
        "\n",
        "filename = '/content/drive/MyDrive/NLP/Assignment-2_laptop/ATE_val.json'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    data_val = json.load(file)\n",
        "\n",
        "data = data_val\n",
        "tokenized_texts = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the items in the parsed JSON object\n",
        "for key, value in data.items():\n",
        "    # Extract text and labels for each item\n",
        "    text = value['text']\n",
        "    label_seq = value['labels']\n",
        "\n",
        "    # Tokenize the text (if needed)\n",
        "    tokenized_text = text.split()  # Using simple split for illustration\n",
        "\n",
        "#     Store the tokenized text\n",
        "    tokenized_texts.append(tokenized_text)\n",
        "#     tokenized_texts.append(text)\n",
        "    # Store the labels\n",
        "    labels.append(label_seq)\n",
        "\n",
        "val_data = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    pair = [tokenized_texts[i], labels[i]]\n",
        "    val_data.append(pair)\n",
        "\n",
        "data = data_test\n",
        "tokenized_texts = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the items in the parsed JSON object\n",
        "for key, value in data.items():\n",
        "    # Extract text and labels for each item\n",
        "    text = value['text']\n",
        "    label_seq = value['labels']\n",
        "\n",
        "    # Tokenize the text (if needed)\n",
        "    tokenized_text = text.split()  # Using simple split for illustration\n",
        "\n",
        "#     Store the tokenized text\n",
        "    tokenized_texts.append(tokenized_text)\n",
        "#     tokenized_texts.append(text)\n",
        "    # Store the labels\n",
        "    labels.append(label_seq)\n",
        "\n",
        "test_data = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    pair = [tokenized_texts[i], labels[i]]\n",
        "    test_data.append(pair)\n",
        "\n",
        "data = data_train\n",
        "tokenized_texts = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the items in the parsed JSON object\n",
        "for key, value in data.items():\n",
        "    # Extract text and labels for each item\n",
        "    text = value['text']\n",
        "    label_seq = value['labels']\n",
        "\n",
        "    # Tokenize the text (if needed)\n",
        "    tokenized_text = text.split()  # Using simple split for illustration\n",
        "\n",
        "#     Store the tokenized text\n",
        "    tokenized_texts.append(tokenized_text)\n",
        "#     tokenized_texts.append(text)\n",
        "    # Store the labels\n",
        "    labels.append(label_seq)\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    pair = [tokenized_texts[i], labels[i]]\n",
        "    train_data.append(pair)"
      ],
      "metadata": {
        "id": "T1GvVI4C-8TE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def calculate_f1_score(model, dataset):\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    for sentence, tags in dataset:\n",
        "        sentence_in = []\n",
        "        for word in sentence:\n",
        "            if word in fasttext_embed.key_to_index:\n",
        "                sentence_in.append(torch.tensor(fasttext_embed.key_to_index[word], dtype=torch.long))\n",
        "            else:\n",
        "                sentence_in.append(torch.tensor(0, dtype=torch.long))\n",
        "        sentence_in = torch.stack(sentence_in)\n",
        "        _, predicted_tages = model(sentence_in)\n",
        "        all_predictions.extend(predicted_tages)\n",
        "        all_targets.extend(tags)\n",
        "    print(\"Accuracy: \", accuracy_score(all_targets, all_predictions))\n",
        "    return f1_score(all_targets, all_predictions, average='macro')\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {}\n",
        "for tag_sen in labels:\n",
        "    for tag in tag_sen:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
        "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
        "print(tag_to_ix)\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 40\n",
        "\n",
        "#train_set = train_data\n",
        "#val_set = val_data\n",
        "test_set = test_data\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_f1_scores = []\n",
        "val_f1_scores = []\n",
        "\n",
        "# Precompute embeddings for all words in the vocabulary\n",
        "embedding_cache = {}\n",
        "for word, idx in fasttext_embed.key_to_index.items():\n",
        "    embedding_cache[word] = torch.tensor(idx, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykWOIi6g_y4y",
        "outputId": "df4ccd88-7a65-4b20-a61a-5d029273d1aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'O': 0, 'B': 1, 'I': 2, '<START>': 3, '<STOP>': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = BiLSTM_CRF(len(fasttext_embed.key_to_index), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, fasttext_embed)\n",
        "device = torch.device('cpu')\n",
        "model_path = '/content/drive/MyDrive/A2_19/Part-3/bilstm_ner_fasttext.pt'\n",
        "model1.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "test_f1 = calculate_f1_score(model1, test_data)\n",
        "print(f'Test F1 Score: {test_f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpOwyseU_2Vk",
        "outputId": "996d3c41-9c20-4295-a945-1a345bd16425"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9303681442524417\n",
            "Test F1 Score: 0.5255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = BiLSTM_CRF(len(fasttext_embed.key_to_index), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, fasttext_embed)\n",
        "device = torch.device('cpu')\n",
        "model_path = '/content/drive/MyDrive/A2_19/Part-3/bilstm_ate_fasttext.pt'\n",
        "model2.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "test_f1 = calculate_f1_score(model2, test_data)\n",
        "print(f'Test F1 Score: {test_f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9OB6tqKAtxx",
        "outputId": "5c51c570-d41b-448c-b1aa-736579b7ef7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9127828273061304\n",
            "Test F1 Score: 0.7083\n"
          ]
        }
      ]
    }
  ]
}